{
  "model": "attention",
  "data_dim": 32,
  "seq_len": 1,
  "batch_size": 32,
  "dataset": "UCF-101",
  "lr": 0.0005,
  "log_dir": "experiments/attention-1624784471"
}